import streamlit as st
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# ğŸ”¹ Load FAISS database
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_store = FAISS.load_local("faiss_index", embedding_model, allow_dangerous_deserialization = True)

# ğŸ”¹ DÃ¹ng mÃ´ hÃ¬nh Ollama (Mistral, Gemma, LLaMA...)
llm = Ollama(model="mistral")

# ğŸ”¹ Táº¡o chatbot RAG
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vector_store.as_retriever(),
    chain_type="stuff",
)

# ğŸ”¹ Giao diá»‡n Streamlit
st.set_page_config(page_title="RAG Chatbot vá»›i Ollama", layout="centered")
st.title("ğŸ¤– RAG Chatbot - Há»i ÄÃ¡p tá»« TÃ i Liá»‡u ğŸ“„")

query = st.text_input("Nháº­p cÃ¢u há»i cá»§a báº¡n:", "")

if st.button("Gá»­i"):
    if query:
        print(query)
        response = rag_chain.invoke({"query": query})
        print(response)
        st.write(f"**Chatbot:** {response}")
    else:
        st.warning("Vui lÃ²ng nháº­p cÃ¢u há»i!")

# Cháº¡y báº±ng lá»‡nh: streamlit run app.py
