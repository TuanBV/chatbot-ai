from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# ðŸ”¹ Load FAISS database
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_store = FAISS.load_local("faiss_index", embedding_model)

# ðŸ”¹ DÃ¹ng mÃ´ hÃ¬nh Ollama (Mistral, Gemma, LLaMA...)
llm = Ollama(model="mistral")  # Báº¡n cÃ³ thá»ƒ thay "mistral" báº±ng "gemma", "phi", v.v.

# ðŸ”¹ Táº¡o chatbot RAG
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vector_store.as_retriever(),
    chain_type="stuff",
)

# ðŸ”¹ VÃ²ng láº·p chat
print("âœ… RAG Chatbot Ä‘Ã£ sáºµn sÃ ng! Nháº­p 'exit' Ä‘á»ƒ thoÃ¡t.")
while True:
    query = input("Báº¡n: ")
    if query.lower() == "exit":
        print("Chatbot: Táº¡m biá»‡t!")
        break
    response = rag_chain.run(query)
    print(f"Chatbot: {response}")
